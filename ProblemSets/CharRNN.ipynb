{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CharRNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usm-cos-432/InClass/blob/master/ProblemSets/CharRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qml7Iv9sKwYG"
      },
      "source": [
        "#COS 432 Character level RNN\n",
        "\n",
        "The source is based on https://github.com/spro/char-rnn.pytorch\n",
        "and \n",
        "https://colab.research.google.com/drive/1ezg4K2VBe2BqmMd43XGukMESExF3wgDM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsvMU7Q7KwzP"
      },
      "source": [
        "from os import path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKc2CJilRWYR"
      },
      "source": [
        "#!pip install -q tqdm\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOGCa5JDK17n"
      },
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "import torchvision.transforms as transforms\n",
        "from IPython import display\n",
        "import time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4kame7P215c"
      },
      "source": [
        "use_cuda = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK3wy8XmLM1d"
      },
      "source": [
        "# Dataset\n",
        "Download Shakespeare, preprocess and Display some examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52KWKp--LRVH"
      },
      "source": [
        "import requests\n",
        "import string\n",
        "import random\n",
        "\n",
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "\n",
        "def DownloadFile(url):\n",
        "    local_filename = url.split('/')[-1]\n",
        "    r = requests.get(url)\n",
        "    return r.text\n",
        "\n",
        "def char_tensor(string):\n",
        "    tensor = torch.zeros(len(string)).long()\n",
        "    for c in range(len(string)):\n",
        "        try:\n",
        "            tensor[c] = all_characters.index(string[c])\n",
        "        except:\n",
        "            continue\n",
        "    return tensor  \n",
        "\n",
        "def random_training_set(chunk_len, batch_size, file):\n",
        "    inp = torch.LongTensor(batch_size, chunk_len)\n",
        "    target = torch.LongTensor(batch_size, chunk_len)\n",
        "    for bi in range(batch_size):\n",
        "        start_index = random.randint(0, len(file) - chunk_len -1)\n",
        "        end_index = start_index + chunk_len + 1\n",
        "        chunk = file[start_index:end_index]\n",
        "        inp[bi] = char_tensor(chunk[:-1])\n",
        "        target[bi] = char_tensor(chunk[1:])\n",
        "    inp = Variable(inp)\n",
        "    target = Variable(target)\n",
        "    if use_cuda:\n",
        "        inp = inp.cuda()\n",
        "        target = target.cuda()\n",
        "    return inp, target\n",
        "\n",
        "def time_since(since):\n",
        "    s = time.time() - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "  \n",
        "target_url = \"https://raw.githubusercontent.com/cos495/code/master/shakespeare.txt\"\n",
        "data = DownloadFile(target_url)\n",
        "#print(random_training_set(10, 8, data))\n",
        "print(data[10:100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw7E1yvHPbMD"
      },
      "source": [
        "#Model\n",
        "In this code we use Pytorch already implemented Recurrent Neural Network Cell computation with `nn.RNN` and `nn.LSTM`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM-RJy9gNcT3"
      },
      "source": [
        "# https://github.com/spro/char-rnn.pytorch\n",
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, model=\"rnn\", n_layers=1):\n",
        "        super(CharRNN, self).__init__()\n",
        "        self.model = model.lower()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
        "        self.rnn = nn.RNN(hidden_size, hidden_size, n_layers)\n",
        "        if model==\"lstm\":\n",
        "          self.rnn = nn.LSTM(hidden_size, hidden_size, n_layers)\n",
        "          \n",
        "        self.decoder = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        batch_size = input.size(0)\n",
        "        encoded = self.encoder(input)\n",
        "        output, hidden = self.rnn(encoded.view(1, batch_size, -1), hidden)\n",
        "        output = self.decoder(output.view(batch_size, -1))\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        if self.model == \"lstm\":\n",
        "            return (Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)),\n",
        "                    Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size)))\n",
        "        return Variable(torch.zeros(self.n_layers, batch_size, self.hidden_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjolCWdP1fkS"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB9xsWiEQhkU"
      },
      "source": [
        "###Iinitialize the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDV_0GRbLtXw"
      },
      "source": [
        "hidden_size = 100\n",
        "learning_rate = 0.01\n",
        "cell = \"rnn\"\n",
        "n_layers = 2\n",
        "\n",
        "decoder = CharRNN(\n",
        "    n_characters,\n",
        "    hidden_size,\n",
        "    n_characters,\n",
        "    model=cell,\n",
        "    n_layers=n_layers,\n",
        ")\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "if use_cuda:\n",
        "    decoder.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRdaN2-FRndV"
      },
      "source": [
        "n_epochs = 2000\n",
        "chunk_len = 200\n",
        "print_every = 100\n",
        "batch_size = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KPmkvGJPkNt"
      },
      "source": [
        "def train(inp, target):\n",
        "    hidden = decoder.init_hidden(batch_size)\n",
        "    if use_cuda:\n",
        "        hidden = hidden.cuda()\n",
        "    decoder.zero_grad()\n",
        "    loss = 0\n",
        "\n",
        "    for c in range(chunk_len):\n",
        "        output, hidden = decoder(inp[:,c], hidden)\n",
        "        loss += criterion(output.view(batch_size, -1), target[:,c])\n",
        "        \n",
        "\n",
        "    loss.backward()\n",
        "#    print(type(loss))\n",
        "    decoder_optimizer.step()\n",
        "#    print(loss.data, chunk_len)\n",
        "#    return loss.data[0] / chunk_len\n",
        "    return loss.item() / chunk_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8M-FSO-TTLF"
      },
      "source": [
        "# Generate Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gh4cIaRDS23s"
      },
      "source": [
        "def generate(decoder, prime_str='A', predict_len=100, temperature=0.8, cuda=False):\n",
        "    hidden = decoder.init_hidden(1)\n",
        "    prime_input = Variable(char_tensor(prime_str).unsqueeze(0))\n",
        "\n",
        "    if cuda:\n",
        "        hidden = hidden.cuda()\n",
        "        prime_input = prime_input.cuda()\n",
        "    predicted = prime_str\n",
        "\n",
        "    # Use priming string to \"build up\" hidden state\n",
        "    for p in range(len(prime_str) - 1):\n",
        "        _, hidden = decoder(prime_input[:,p], hidden)\n",
        "        \n",
        "    inp = prime_input[:,-1]\n",
        "    \n",
        "    for p in range(predict_len):\n",
        "        output, hidden = decoder(inp, hidden)\n",
        "        \n",
        "        # Sample from the network as a multinomial distribution\n",
        "        output_dist = output.data.view(-1).div(temperature).exp()\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "\n",
        "        # Add predicted character to string and use as next input\n",
        "        predicted_char = all_characters[top_i]\n",
        "        predicted += predicted_char\n",
        "        inp = Variable(char_tensor(predicted_char).unsqueeze(0))\n",
        "        if cuda:\n",
        "            inp = inp.cuda()\n",
        "\n",
        "    return predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riXDDbny1-DF"
      },
      "source": [
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        "\n",
        "print(\"Training for %d epochs...\" % n_epochs)\n",
        "for epoch in tqdm(range(1, n_epochs + 1)):\n",
        "    loss = train(*random_training_set(chunk_len, batch_size, data))\n",
        "    loss_avg += loss\n",
        "\n",
        "    if epoch % print_every == 0:\n",
        "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
        "        print('loss: ', loss)\n",
        "        print(generate(decoder, 'Wh', 100, cuda=use_cuda), '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar1_B7egYZWK"
      },
      "source": [
        "torch.save(decoder,'rnnTrainedModel.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7snNirbPzaDk"
      },
      "source": [
        "\n",
        "### Let's try sampling with high temperature:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oCPS6kzRK4w"
      },
      "source": [
        "generate(decoder, prime_str=\"A\", temperature= 100, cuda=use_cuda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1g9IOY-zmZ8"
      },
      "source": [
        "### Let's try sampling with low temperature:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77ijhzn9Tc1C"
      },
      "source": [
        "generate(decoder, prime_str=\"A\", temperature= 0.5, cuda=use_cuda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyFpPpgJzss4"
      },
      "source": [
        "### Describe the difference\n",
        "How do the samples qualitatively change? What does changing the temperature do to distribution of possible outputs?Â¶\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xND5BVvo1pxa"
      },
      "source": [
        "### Starting Prompts\n",
        "Explain how the model uses the `prime_str` to generate new text. Find a good prime string and display the results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkfQIbosztV_"
      },
      "source": [
        "generate(decoder, prime_str=\"why\", cuda=use_cuda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvU9zelr7ivL"
      },
      "source": [
        "#Cell Types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0izKDP403t2"
      },
      "source": [
        "### LSTM Gates\n",
        "Explain the role of the LSTM gates (you may this article useful : https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "\n",
        "\n",
        "$i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\\n",
        "f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\\n",
        "g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\\\\n",
        "o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\\n",
        "c_t = f_t c_{(t-1)} + i_t g_t \\\\\n",
        "h_t = o_t \\tanh(c_t)$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4rEoPcD1CxE"
      },
      "source": [
        "here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCPJRlYtzCXG"
      },
      "source": [
        "### Explain how LSTM Cell is different than Simple RNN? (why is it better or worse?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP86OZ3b1Eyy"
      },
      "source": [
        "here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe9TPf__7yaJ"
      },
      "source": [
        "###Train CharRNN with your LSTM cell. Compare the results to the Simple rnn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3onffCP7wxs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}