{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "multivariable-calculus-Activity.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usm-cos-432/InClass/blob/master/chapter2/multivariable_calculus_Activity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V555wYa8Dh-Y",
        "colab_type": "text"
      },
      "source": [
        "### Multivariate Chain Rule \n",
        "\n",
        "In this activity, you will use an example in Section 18.4 of the book as a guide for calculating gradients in small neural network.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 8,
        "id": "0OfgpAliDh-Z",
        "colab_type": "text"
      },
      "source": [
        "### Example of multivariate chain rule applied to a small neural network \n",
        "Let us suppose that we have a function of four variables ($w, x, y$, and $z$) which we can make by composing many terms:\n",
        "\n",
        "$$\\begin{aligned}f(u, v) & = (u+v)^{2} \\\\u(a, b) & = (a+b)^{2}, \\qquad v(a, b) = (a-b)^{2}, \\\\a(w, x, y, z) & = (w+x+y+z)^{2}, \\qquad b(w, x, y, z) = (w+x-y-z)^2.\\end{aligned}$$\n",
        "\n",
        "\n",
        "Such chains of equations are common when working with neural networks, so trying to understand how to compute gradients of such functions is key.  We can start to see visual hints of this connection in :numref:`fig_chain-1` if we take a look at what variables directly relate to one another.\n",
        "\n",
        "![The function relations above where nodes represent values and edges show functional dependence.](http://www.cs.usm.maine.edu/~macleod/courses/cos432/inClass/img/ChainNet1.svg)\n",
        ":label:`fig_chain-1`\n",
        "\n",
        "Nothing stops us from just composing everything from :eqref:`eq_multi_func_def` and writing out that\n",
        "\n",
        "$$\n",
        "f(w, x, y, z) = \\left(\\left((w+x+y+z)^2+(w+x-y-z)^2\\right)^2+\\left((w+x+y+z)^2-(w+x-y-z)^2\\right)^2\\right)^2.\n",
        "$$\n",
        "\n",
        "We may then take the derivative by just using single variable derivatives, but if we did that we would quickly find ourself swamped with terms, many of which are repeats!  Indeed, one can see that, for instance:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial f}{\\partial w} & = 2 \\left(2 \\left(2 (w + x + y + z) - 2 (w + x - y - z)\\right) \\left((w + x + y + z)^{2}- (w + x - y - z)^{2}\\right) + \\right.\\\\\n",
        "& \\left. \\quad 2 \\left(2 (w + x - y - z) + 2 (w + x + y + z)\\right) \\left((w + x - y - z)^{2}+ (w + x + y + z)^{2}\\right)\\right) \\times \\\\\n",
        "& \\quad \\left(\\left((w + x + y + z)^{2}- (w + x - y - z)^2\\right)^{2}+ \\left((w + x - y - z)^{2}+ (w + x + y + z)^{2}\\right)^{2}\\right).\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "If we then also wanted to compute $\\frac{\\partial f}{\\partial x}$, we would end up with a similar equation again with many repeated terms, and many *shared* repeated terms between the two derivatives.  This represents a massive quantity of wasted work, and if we needed to compute derivatives this way, the whole deep learning revolution would have stalled out before it began!\n",
        "\n",
        "\n",
        "Let us break up the problem.  We will start by trying to understand how $f$ changes when we change $a$, essentially assuming that $w, x, y$, and $z$ all do not exist.  We will reason as we did back when we worked with the gradient for the first time.  Let us take $a$ and add a small amount $\\epsilon$ to it. \n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& f(u(a+\\epsilon, b), v(a+\\epsilon, b)) \\\\\n",
        "\\approx & f\\left(u(a, b) + \\epsilon\\frac{\\partial u}{\\partial a}(a, b), v(a, b) + \\epsilon\\frac{\\partial v}{\\partial a}(a, b)\\right) \\\\\n",
        "\\approx & f(u(a, b), v(a, b)) + \\epsilon\\left[\\frac{\\partial f}{\\partial u}(u(a, b), v(a, b))\\frac{\\partial u}{\\partial a}(a, b) + \\frac{\\partial f}{\\partial v}(u(a, b), v(a, b))\\frac{\\partial v}{\\partial a}(a, b)\\right].\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "The first line follows from the definition of partial derivative, and the second follows from the definition of gradient.  It is notationally burdensome to track exactly where we evaluate every derivative, as in the expression $\\frac{\\partial f}{\\partial u}(u(a, b), v(a, b))$, so we often abbreviate this to the much more memorable\n",
        "\n",
        "$$\n",
        "\\frac{\\partial f}{\\partial a} = \\frac{\\partial f}{\\partial u}\\frac{\\partial u}{\\partial a}+\\frac{\\partial f}{\\partial v}\\frac{\\partial v}{\\partial a}.\n",
        "$$\n",
        "\n",
        "It is useful to think about the meaning of the process. We are trying to understand how a function of the form $f(u(a, b), v(a, b))$ changes its value with a change in $a$.  There are two pathways this can occur: there is the pathway where $a \\rightarrow u \\rightarrow f$ and where $a \\rightarrow v \\rightarrow f$.  We can compute both of these contributions via the chain rule: $\\frac{\\partial w}{\\partial u} \\cdot \\frac{\\partial u}{\\partial x}$ and $\\frac{\\partial w}{\\partial v} \\cdot \\frac{\\partial v}{\\partial x}$ respectively, and added up.  \n",
        "\n",
        "To calculate $\\frac{\\partial f}{\\partial w}$ that we can write\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial f}{\\partial w} & = \\frac{\\partial f}{\\partial a}\\frac{\\partial a}{\\partial w} + \\frac{\\partial f}{\\partial b}\\frac{\\partial b}{\\partial w}, \\\\\n",
        "\\frac{\\partial f}{\\partial a} & = \\frac{\\partial f}{\\partial u}\\frac{\\partial u}{\\partial a}+\\frac{\\partial f}{\\partial v}\\frac{\\partial v}{\\partial a}, \\\\\n",
        "\\frac{\\partial f}{\\partial b} & = \\frac{\\partial f}{\\partial u}\\frac{\\partial u}{\\partial b}+\\frac{\\partial f}{\\partial v}\\frac{\\partial v}{\\partial b}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Note that this application of the chain rule has us explicitly compute $\\frac{\\partial f}{\\partial u}, \\frac{\\partial f}{\\partial v}, \\frac{\\partial f}{\\partial a}, \\frac{\\partial f}{\\partial b}, \\; \\text{and} \\; \\frac{\\partial f}{\\partial w}$.  Nothing stops us from also including the equations:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial f}{\\partial x} & = \\frac{\\partial f}{\\partial a}\\frac{\\partial a}{\\partial x} + \\frac{\\partial f}{\\partial b}\\frac{\\partial b}{\\partial x}, \\\\\n",
        "\\frac{\\partial f}{\\partial y} & = \\frac{\\partial f}{\\partial a}\\frac{\\partial a}{\\partial y}+\\frac{\\partial f}{\\partial b}\\frac{\\partial b}{\\partial y}, \\\\\n",
        "\\frac{\\partial f}{\\partial z} & = \\frac{\\partial f}{\\partial a}\\frac{\\partial a}{\\partial z}+\\frac{\\partial f}{\\partial b}\\frac{\\partial b}{\\partial z}.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "and then keeping track of how $f$ changes when we change *any* node in the entire network.  Let us implement it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 11,
        "tab": [
          "pytorch"
        ],
        "id": "Xx0Ou6SEDh-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute the value of the function from inputs to outputs\n",
        "w, x, y, z = -1, 0, -2, 1\n",
        "a, b = (w + x + y + z)**2, (w + x - y - z)**2\n",
        "u, v = (a + b)**2, (a - b)**2\n",
        "f = (u + v)**2\n",
        "print(f'f at {w}, {x}, {y}, {z} is {f}')\n",
        "\n",
        "# Compute the derivative using the decomposition above\n",
        "# First compute the single step partials\n",
        "df_du, df_dv = 2*(u + v), 2*(u + v)\n",
        "du_da, du_db, dv_da, dv_db = 2*(a + b), 2*(a + b), 2*(a - b), -2*(a - b)\n",
        "da_dw, db_dw = 2*(w + x + y + z), 2*(w + x - y - z)\n",
        "da_dx, db_dx = 2*(w + x + y + z), 2*(w + x - y - z)\n",
        "da_dy, db_dy = 2*(w + x + y + z), -2*(w + x - y - z)\n",
        "da_dz, db_dz = 2*(w + x + y + z), -2*(w + x - y - z)\n",
        "\n",
        "# Now compute how f changes when we change any value from output to input\n",
        "df_da, df_db = df_du*du_da + df_dv*dv_da, df_du*du_db + df_dv*dv_db\n",
        "df_dw, df_dx = df_da*da_dw + df_db*db_dw, df_da*da_dx + df_db*db_dx\n",
        "df_dy, df_dz = df_da*da_dy + df_db*db_dy, df_da*da_dz + df_db*db_dz\n",
        "\n",
        "print(f'df/dw at {w}, {x}, {y}, {z} is {df_dw}')\n",
        "print(f'df/dx at {w}, {x}, {y}, {z} is {df_dx}')\n",
        "print(f'df/dy at {w}, {x}, {y}, {z} is {df_dy}')\n",
        "print(f'df/dz at {w}, {x}, {y}, {z} is {df_dz}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 12,
        "id": "AxRaHDdKDh-d",
        "colab_type": "text"
      },
      "source": [
        "The fact that we compute derivatives from $f$ back towards the inputs rather than from the inputs forward to the outputs (as we did in the first code snippet above) is what gives this algorithm its name: *backpropagation*.  Note that there are two steps:\n",
        "1. Compute the value of the function, and the single step partials from front to back.  While not done above, this can be combined into a single *forward pass*.\n",
        "2. Compute the gradient of $f$ from back to front.  We call this the *backwards pass*.\n",
        "\n",
        "This is precisely what every deep learning algorithm implements to allow the computation of the gradient of the loss with respect to every weight in the network at one pass.  It is an astonishing fact that we have such a decomposition.\n",
        "\n",
        "All of what we did above can be done automatically by calling f.backwards() in pytorch. To see how, let us take a quick look at this example. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 14,
        "tab": [
          "pytorch"
        ],
        "id": "FriMLZftDh-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "# Initialize as ndarrays, then attach gradients\n",
        "w = torch.tensor([-1.], requires_grad=True)\n",
        "x = torch.tensor([0.], requires_grad=True) \n",
        "y = torch.tensor([-2.], requires_grad=True)\n",
        "z = torch.tensor([1.], requires_grad=True)\n",
        "# Do the computation like usual, tracking gradients\n",
        "a, b = (w + x + y + z)**2, (w + x - y - z)**2\n",
        "u, v = (a + b)**2, (a - b)**2\n",
        "f = (u + v)**2\n",
        "\n",
        "# Execute backward pass\n",
        "f.backward()\n",
        "\n",
        "print(f'df/dw at {w.data.item()}, {x.data.item()}, {y.data.item()}, '\n",
        "      f'{z.data.item()} is {w.grad.data.item()}')\n",
        "print(f'df/dx at {w.data.item()}, {x.data.item()}, {y.data.item()}, '\n",
        "      f'{z.data.item()} is {x.grad.data.item()}')\n",
        "print(f'df/dy at {w.data.item()}, {x.data.item()}, {y.data.item()}, '\n",
        "      f'{z.data.item()} is {y.grad.data.item()}')\n",
        "print(f'df/dz at {w.data.item()}, {x.data.item()}, {y.data.item()}, '\n",
        "      f'{z.data.item()} is {z.grad.data.item()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 8,
        "id": "1v3CwdspDh-g",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## Multivariate Chain Rule : Round 2\n",
        "Let us suppose that we have a function of four variables ($w, x, y$, and $z$) which we can make by composing many terms:\n",
        "\n",
        "$$\\begin{aligned}f(u, v) & = (u^{2}+3*v) \\\\u(a, b) & = (2*a+b^2), \\qquad v(a, b) = (4*a-b^3), \\\\a(w, x, y, z) & = (w+x+y+z)^{2}, \\qquad b(w, x, y, z) = (3*w+2*x-y-z).\\end{aligned}$$\n",
        "\n",
        "\n",
        "Follow the above analysis by first computing the gradients manually and then computing them using pytorch's autograd and backpropogation capabilities \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyEdHBIhDh-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JG5ppyJ5Dh-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Do the gradients make sense given the equations in the network ?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSZNzfxwDh-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr-miZUKDh-m",
        "colab_type": "text"
      },
      "source": [
        "Evaluate the network using a different set of input values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmbgjUSuDh-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RG-bFn_kDh-p",
        "colab_type": "text"
      },
      "source": [
        "All of what we did above can be done automatically by calling f.backwards()."
      ]
    }
  ]
}